<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/bulma@1.0.2/css/bulma.min.css"
    > -->
    <link rel="stylesheet" href="assets/css/bulma-no-dark-mode.css" />
    <link rel="stylesheet" href="assets/css/styles.css" />
    <title>DeSTA2: Developing Instruction-Following Speech Language Model Without Speech Instruction-Tuning Data</title>

    <section class="hero">
      <div class="container">
        <div class="hero-body">
          <p class="title is-1">
            DeSTA2: Developing Instruction-Following Speech Language Model Without Speech Instruction-Tuning Data
          </p>
          <div class="content is-large">
            <p class="subtitle">
              Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank
              Wang, Hung-yi Lee
            </p>
            <p class="subtitle">National Taiwan University, NVIDIA</p>

            <a href="https://arxiv.org/pdf/2409.20007">üìë Paper</a> |
            <a href="https://kehanlu.github.io/DeSTA2/">üåê Website</a> |
            <a href="https://github.com/kehanlu/DeSTA2">üë©‚Äçüíª Github</a> |
            <a href="https://huggingface.co/DeSTA-ntu/DeSTA2-8B-beta">ü§ó Model</a> |
            <a href="https://huggingface.co/datasets/DeSTA-ntu/DeSTA2-Llama3-8B-Instruct">ü§ó Dataset</a>

            <!-- <p>Recent end-to-end speech language models (SLMs) have expanded upon the capabilities of large language
            models (LLMs) by incorporating pre-trained speech models. However, these SLMs often undergo
            extensive speech instruction-tuning to bridge the gap between speech and text modalities. This
            requires significant annotation efforts and risks catastrophic forgetting of the original language
            capabilities. In this work, we present a simple yet effective automatic process for creating
            speech-text pair data that carefully injects speech paralinguistic understanding abilities into SLMs
            while preserving the inherent language capabilities of the text-based LLM. Our model demonstrates
            general capabilities for speech-related tasks without the need for speech instruction tuning data,
            achieving impressive performance on Dynamic-SUPERB and AIR-Bench-Chat benchmarks. Furthermore, our
            model exhibits the ability to follow complex instructions derived from LLMs, such as specific output
            formatting and chain-of-thought reasoning. Our approach not only enhances the versatility and
            effectiveness of SLMs but also reduces reliance on extensive annotated datasets, paving the way for
            more efficient and capable speech understanding systems.
          </p> -->
          </div>
          <img src="assets/images/figure1.png" style="border-radius: 50px" alt="" />
        </div>
      </div>
    </section>
    <!-- <section class="section">
    <div class="container">
      <img src="assets/images/figure1.png" style="border-radius: 50px;" alt="">

    </div>
  </section> -->

    <section class="section">
      <div class="container">
        <div class="box">
          <div class="content is-large">
            <h2 class="title is-4">‚ú®‚ú® Key features</h2>

            <ul>
              <li>
                We propose an automatic approach to develop speech language models <em><b>without</b></em> using speech
                instruction-tuning data. We aim to inject speech understanding capability into text-based LLMs while
                keeping their original versatility.
              </li>
              <li>
                We train the end-to-end speech language model with carefully constructed
                <a href="#dataset-construction">speech captions</a> to avoid the catastrophic forgetting problem.
              </li>
              <li>
                The model shows strong <a href="#instruction-following">instruction-following ability</a> and
                demonstrates robust speech understanding capability on the
                <a href="#dynamic-superb">Dynamic-SUPERB</a> and
                <a href="#AIR-Bench-Chat">AIR-Bench-Chat</a> benchmarks.
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <div class="box">
          <div class="content">
            <h2 class="title is-2" id="instruction-following">Instruction-following Ability</h2>
            <p>
              Our model not only demonstrates strong speech understanding capabilities but also excels in following
              complex instructions, such as following specific output formatting and chain-of-thought reasoning.
            </p>
          </div>

          <audio controls>
            <source src="assets/audios/7_1_d7.wav" type="audio/ogg" />
          </audio>
          <table class="table is-fullwidth">
            <thead>
              <tr>
                <th>Instruction</th>
                <th>Model Output</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  What is the gender of the speaker?
                  <b>Your entire response should be in English, capital letters only.</b>
                </td>
                <td>THE GENDER OF THE SPEAKER IS FEMALE.</td>
              </tr>
              <tr>
                <td>
                  What is the emotion of the speaker?
                  <b>Your entire response should be in English, capital letters only.</b>
                </td>
                <td>HAPPINESS</td>
              </tr>
              <tr>
                <td>What is the gender of the speaker? <b>Answer in JSON format.</b></td>
                <td>{"gender": "Female" }</td>
              </tr>
              <tr>
                <td>What is the emotion conveyed in the speech? <b>Think step by step.</b></td>
                <td>
                  Let's break it down step by step!<br />1. We start with the phrase "Thank you for calling". This
                  implies that a request has been made, and the speaker is expressing gratitude.<br />2. The next phrase
                  is "Goodbye". This is a normal way to end a conversation, and it's a polite way to part ways.<br />Considering
                  these two phrases together, we can infer that the speaker is happy to end the conversation and is not
                  expecting anything further. The tone is friendly and grateful, suggesting that the speaker has had a
                  positive interaction.<br />Therefore, the emotion conveyed in the speech is happiness!
                </td>
              </tr>
              <tr>
                <td><b>Analyze the speech,</b> then tell me what is the emotion conveyed in the speech?</td>
                <td>
                  Let's break down the speech:<br />* "Thank you for calling": This phrase is a polite expression of
                  gratitude for the person's call.<br />* "Goodbye": This is a final farewell, indicating that the
                  conversation is ending.<br />The tone of the speech is polite and friendly, as the speaker is thanking
                  the person for calling and bidding them farewell.<br />The emotion conveyed in this speech is
                  happiness. The speaker seems cheerful and appreciative, expressing gratitude for the call and ending
                  the conversation on a positive note.
                </td>
              </tr>
              <tr>
                <td>Calculate the words per second rate, <b>think step by step.</b></td>
                <td>
                  Let's break it down step by step:<br />1. The speech lasts for 2 seconds: [00:00:00 - 00:00:02]<br />2.
                  The number of words is: "Thank you for calling. Goodbye." (5 words)<br />3. To find the words per
                  second rate, we need to divide the number of words by the time it took to say the words:<br />Words
                  per second = Total number of words / Time taken (in seconds)<br />= 5 words / 2 seconds<br />= 2.5
                  words per second<br />Therefore, the words per second rate is 2.5 words
                </td>
              </tr>
              <tr>
                <td>
                  How many speakers are there in the audio?<br />Can you tell me the emotion of each speaker?(sad=1,
                  angry=2, neutral=3, happy=4)?<br />How long is the audio in seconds? <br /><br />Do the math multiply
                  the numbers from above 3 questions. Think step by step.
                </td>
                <td>
                  Let's break it down step by step:<br />
                  1. How many speakers are there in the audio?<br />
                  There is only 1 speaker.<br />
                  2. Can you tell me the emotion of each speaker?<br />
                  The emotion is happiness, which corresponds to a value of 4.<br />
                  3. How long is the audio in seconds?<br />
                  The audio is 2 seconds long.<br />

                  Now, let's do the math:<br />
                  1 x 4 x 2 = 8<br />
                  So, the answer is 8!
                </td>
              </tr>
              <tr>
                <td>
                  How many speakers are there in the audio? Can you tell me the emotion of each speaker? Can you also
                  provide a brief summary of the audio? Is there any background noise? How do you feel today? Can you
                  give me a response to the audio?
                </td>
                <td>
                  There is only 1 speaker in the audio.<br />

                  The speaker is a female, and her emotion is happiness.<br />

                  The audio is a brief goodbye message, where the speaker says "Thank you for calling. Goodbye."<br />

                  There is no background noise in the audio.<br />

                  As a machine, I don't have emotions, so I don't feel anything.<br />

                  As for a response to the audio, I can say: "You're welcome, and have a great day!"
                </td>
              </tr>
              <tr>
                <td>
                  Ë´ãÊèèËø∞ÈÄôÊÆµËÅ≤Èü≥ÔºåËÅ≤Èü≥ËÅΩËµ∑‰æÜÂ¶Ç‰ΩïÔºüÂÖßÂÆπË™™‰∫Ü‰ªÄÈ∫ºÔºü ‰ª•‰∏≠ÊñáÂõûÁ≠î„ÄÇ<br /><br /><em
                    >Note: our model only trained on English data, but it is generalizable to other languages thanks to
                    the multilingual capability of Llama3! üòä
                  </em>
                </td>
                <td>
                  ÈÄôÊÆµËÅ≤Èü≥ÊòØ‰∏ÄÂÄãÂ•≥ÊÄßÁöÑËÅ≤Èü≥ÔºåËÅΩËµ∑‰æÜÊòØÂæàÊÑâÂø´ÁöÑ„ÄÇÂ•πË™™‰∫Ü‰∏ÄÂè•„ÄåThank you for calling.
                  Goodbye„ÄçÔºåË™ûÊ∞£Ê∫´Êüî„ÄÅÊüîÂíåÔºåÊ≤íÊúâ‰ªª‰ΩïÊøÄÂãïÊàñÊøÄÊÉÖÁöÑÊÑüË¶∫„ÄÇÊï¥ÂÄãËÅ≤Èü≥ËÅΩËµ∑‰æÜÂæàÂπ≥Èùú„ÄÅÂæàËá™ÁÑ∂ÔºåÊ≤íÊúâ‰ªª‰ΩïÊòéÈ°ØÁöÑÊÑüË¶∫ÊàñÊÉÖÊÑüËâ≤ÂΩ©„ÄÇ
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2 class="title is-2">Speech Understanding Ability</h2>
        <div class="content is-medium">
          <h2 class="title is-4" id="dynamic-superb">Dynamic-SUPERB</h2>
          <p>
            <a href="https://ieeexplore.ieee.org/document/10448257">Dynamic-SUPERB</a> is a crowdsourced benchmark
            comprising 48 speech-related classification tasks with human-created instructions and answers.
          </p>
        </div>

        <div class="table-container">
          <table class="table is-fullwidth">
            <thead>
              <tr>
                <th>Audio</th>
                <th>Instruction</th>
                <th>Model Output</th>
                <th>Ground Truth</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <audio controls>
                    <source src="assets/audios/LanguageIdentification_VoxForge_0000_de143-43.flac" type="audio/ogg" />
                  </audio>
                  <p>LanguageIdentification</p>
                </td>
                <td>
                  Which language can you recognize from this audio recording? The answer could be German, English,
                  Spanish, Italian, Russian, or French.
                </td>
                <td>
                  Based on the audio recording, I recognize the language as German. The phrase "Die h√§tte man dicht
                  machen m√ºssen" is a grammatically correct and idiomatic German sentence, and the speaker's accent and
                  pronunciation also suggest a German speaker.
                </td>
                <td>German</td>
              </tr>
              <tr>
                <td>
                  <audio controls>
                    <source
                      src="assets/audios/NoiseDetection_LJSpeech_MUSAN-Music_0199_music_LJSpeech-1.1_16k_LJ050-0033.wav"
                      type="audio/ogg"
                    />
                  </audio>
                  <p>NoiseDetection</p>
                </td>
                <td>
                  Determine whether the sentence is a noise-free utterance or a noisy one with music disturbance. The
                  answer could be clean or noisy.
                </td>
                <td>
                  Based on the provided information, I would classify the sentence as "noisy". The noise level is
                  moderate (15db), and there is a reverberation time of 60ms, which suggests that there is some
                  background noise or music disturbance present.
                </td>
                <td>Noisy</td>
              </tr>
              <tr>
                <td>
                  <audio controls>
                    <source
                      src="assets/audios/AccentClassification_AccentdbExtended_0193_british_s01_176.wav"
                      type="audio/ogg"
                    />
                  </audio>
                  <p>AccentClassification</p>
                </td>
                <td>
                  The task involves detecting variations in pronunciation and intonation. The answer could be american,
                  australian, bangla, british, indian, malayalam, odiya, telugu, or welsh.
                </td>
                <td>Based on the accent and pronunciation, I would identify the speaker as having a British accent.</td>
                <td>British</td>
              </tr>
              <tr>
                <td>
                  <audio controls>
                    <source
                      src="assets/audios/EmotionRecognition_MultimodalEmotionlinesDataset_0026_dia382_utt0.wav"
                      type="audio/ogg"
                    />
                  </audio>
                  <p>EmotionRecognition</p>
                </td>
                <td>
                  Recognize and categorize the emotions expressed in spoken language. The answer could be anger,
                  disgust, sadness, joy, neutral, surprise, or fear.
                </td>
                <td>
                  Based on the audio clip, the emotion expressed is "joy". The speaker's tone and words ("Here she is,
                  future fashion superstar") convey a sense of excitement and happiness, indicating a positive emotional
                  state.
                </td>
                <td>Joy</td>
              </tr>
              <tr>
                <td>
                  <audio controls>
                    <source
                      src="assets/audios/DialogueEmotionClassification_DailyTalk_0196_7_1_d756.wav"
                      type="audio/ogg"
                    />
                  </audio>
                  <p>DialogueEmotionClassification</p>
                </td>
                <td>
                  Analyze the dialogue and identify its corresponding emotion. The answer could be anger, disgust, fear,
                  sadness, happiness, surprise, or no emotion.
                </td>
                <td>
                  Based on the dialogue, the corresponding emotion is sadness. The speaker is apologizing and expressing
                  regret for causing trouble, which is a characteristic of a sad tone.
                </td>
                <td>Sadness</td>
              </tr>
              <tr>
                <td>
                  <audio controls>
                    <source src="assets/audios/SpeechTextMatching_LJSpeech_0001_LJ001-0107.wav" />
                  </audio>
                  <p>SpeechTextMatching</p>
                </td>
                <td>
                  Verify if the speech and text are congruent. The answer could be yes or no.<br />Text: "guidelines for
                  an experimental program to develop more detailed criteria"
                </td>
                <td>
                  The answer is NO.<br />The speech is about saying a few words on the principles of design and
                  typography, but the text is about guidelines for an experimental program to develop more detailed
                  criteria, which is a different topic.
                </td>
                <td>No</td>
              </tr>
              <tr>
                <td>
                  <audio controls>
                    <source src="assets/audios/SpeakerVerification_LibriSpeech-TestClean_0046_3575-170457-0038.flac" />
                  </audio>
                  <p>SpeakerVerification</p>
                </td>
                <td>
                  Scrutinize the first audio clip to establish whether the second one is delivered by the same speaker.
                  The answer could be yes or no.
                </td>
                <td>
                  Based on the provided audio clip, I would say NO, the second audio clip is not delivered by the same
                  speaker. The tone, pitch, and emotion expressed in the first audio clip (neutral, calm, and
                  introspective) are different from what I would expect from the same speaker. The first audio clip has
                  a more contemplative and reflective tone, whereas the second audio clip is more dramatic and emotive.
                </td>
                <td>No</td>
              </tr>
            </tbody>
          </table>
        </div>

        <div class="content is-medium">
          <h2 class="title is-4" id="AIR-Bench-Chat">AIR-Bench-Chat</h2>
          <p>
            <a href="https://arxiv.org/abs/2402.07729">AIR-Bench-Chat</a> is an LLM-generated benchmark derived from
            speech metadata that focuses on open-ended questions.
          </p>
        </div>
        <div class="table-container">
          <table class="table is-fullwidth">
            <thead>
              <tr>
                <th>Audio</th>
                <th>Instruction</th>
                <th>Model Output</th>
                <th>Ground Truth</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>
                  <audio controls>
                    <source src="assets/audios/Ses01M_script01_1_F014.wav" />
                  </audio>
                </td>
                <td>Based on the speaker's emotional state, what could be the nature of the proof being requested?</td>
                <td>
                  Based on the speaker's angry tone and high-pitched voice, it is likely that the proof being requested
                  is related to a dispute or controversy. The speaker is demanding evidence to support a claim or
                  argument, and is likely feeling frustrated or upset that the other party has not provided sufficient
                  evidence to back up their claims.
                </td>
                <td>
                  Given the speaker's anger, the proof likely relates to a serious allegation or an issue that
                  significantly impacts the speaker..
                </td>
              </tr>
              <tr>
                <td>
                  <audio controls>
                    <source src="assets/audios/Ses04F_impro02_M004.wav" />
                  </audio>
                </td>
                <td>
                  What emotion is the speaker likely trying to convey to the listener by stating a fear of their
                  departure?
                </td>
                <td>
                  Based on the given information, the speaker is likely trying to convey a sense of sadness to the
                  listener by stating a fear of their departure. The low pitch, slow speaking speed, and very low volume
                  suggest a somber and melancholic tone, which is consistent with a sad emotion.
                </td>
                <td>The speaker is trying to convey a sense of sadness and attachment to the listener.</td>
              </tr>
              <tr>
                <td>
                  <audio controls>
                    <source src="assets/audios/Ses01F_script03_1_F029.wav" />
                  </audio>
                </td>
                <td>What could be the next step for the manager after witnessing such a scene?</td>
                <td>
                  Based on the manager's tone and emotion, it seems that she is amused and entertained by the scene.
                  Given her high pitch and volume, she might be trying to emphasize the absurdity of the situation.
                  Considering her slow speaking speed, she might be savoring the moment and enjoying the spectacle.<br /><br />As
                  a next step, the manager could:<br />* Laugh and join in on the fun, possibly even taking a photo or
                  video to capture the moment.<br />* Try to intervene and calm the situation down, but in a playful and
                  lighthearted manner, acknowledging the absurdity of the situation.<br />* Use the opportunity to make
                  a humorous
                </td>
                <td>The manager might need to intervene or address the situation occurring on the floor.</td>
              </tr>
              <tr>
                <td>
                  <audio controls>
                    <source src="assets/audios/MUL0608_120.98_148.92.wav" />
                  </audio>
                </td>
                <td>What did the second person inquire about after the first person's request?</td>
                <td>The second person inquired about the price range of the restaurant.</td>
                <td>The second person inquired about any specifications on the price range.</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h1 class="title is-2">Method</h1>
        <div class="content is-large">
          <h2 class="title is-3" id="dataset-construction">Stage 1: Dataset Consturction</h2>

          <p>
            In our dataset construction process, we first extract and format all textual attributes from the speech into
            a structured "seed transcript." This seed transcript is then combined with the prompt "What can you hear
            from the audio?" and fed into Llama3. The response generated by Llama3 becomes our training target for the
            end-to-end speech language model.
          </p>
          <p>
            There are 12 attributes, including gender, age, accent, emotion, pitch, volume, speaking speed, SNR level,
            C50 value, duration, intent, and spoken text.
          </p>

          <img src="assets/images/dataset_construction.png" alt="" />
        </div>
      </div>

      <div class="container">
        <!-- 
      <div class="content">
        <code><|begin_of_text|><|start_header_id|>user<|end_header_id|> </code><br>
        <code><b>{seed transcript}</b></code><br>

        <code><b>What can you hear from the audio?</b></code><br>
        <code><|eot_id|><|start_header_id|>assistant<|end_header_id|></code>
      </div> -->

        <table class="table is-fullwidth">
          <thead>
            <tr>
              <th style="color: orange">Seed Transcript</th>
              <th>Llama3-Instruct Response</th>
              <th>Correspond Audio</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>
                [00:00:01 - 00:00:02]: "These days are long gone." (Gender:Male, Accent: Scottish, Age: 20, Emotion:
                neutral, Duration: 2s)
              </td>
              <td>
                From the audio, I can hear:<br />* A male voice with a Scottish accent<br />* The voice is approximately
                20 years old<br />* The tone is neutral, indicating a lack of strong emotions<br />* The audio duration
                is 2 seconds
              </td>
              <td>
                <audio controls>
                  <source src="assets/audios/p284_159.wav" />
                </audio>
                <p>VCTK-Corpus</p>
              </td>
            </tr>
            <tr>
              <td>
                [00:00:01 - 00:00:04]: "At the top level, the salaries are far too high." (Gender:Male, Accent: English,
                Age: 23, Emotion: neutral, Duration: 4s)
              </td>
              <td>
                From the audio, we can hear a male speaker with an English accent (possibly British English) saying:<br />"At
                the top level, the salaries are far too high."<br />The speaker is a 23-year-old male with a neutral
                emotion. The audio lasts for 4 seconds.
              </td>
              <td>
                <audio controls>
                  <source src="assets/audios/p287_162.wav" />
                </audio>
                <p>VCTK-Corpus</p>
              </td>
            </tr>
            <tr>
              <td>
                [00:00:01 - 00:00:04]: "Oh, lovely. Where's the wedding going to be?" (Gender:Male, Emotion: happiness,
                Action: question, Duration: 4s)
              </td>
              <td>
                From the audio, I can hear a male speaker asking a question with a tone of happiness, specifically the
                words: "Oh, lovely. Where's the wedding going to be?"
              </td>
              <td>
                <audio controls>
                  <source src="assets/audios/4_0_d47.wav" />
                </audio>
                <p>Dailytalk</p>
              </td>
            </tr>
            <tr>
              <td>
                [00:00:01 - 00:00:07]: "A new pride taught me mine ego, and that teach I unto men: no longer to thrust
                one's head into the sand of celestial things, but to carry it freely, a terrestrial head, which giveth
                meaning to the earth!" (Gender:Male, Emotion: cheerful, Pitch: Normal, Volume: Normal, Speaking speed:
                Fast, Duration: 7s)
              </td>
              <td>
                From the given audio, we can hear a male speaker with a cheerful tone, speaking at a normal pitch and
                volume. The speaking speed is relatively fast, and the duration is 7 seconds.<br />The audio appears to
                be a philosophical or inspirational quote, with the speaker using poetic language to convey a message.
                The text itself is a condensed and somewhat abstract statement, but it seems to be encouraging people to
                be more grounded and down-to-earth, rather than getting lost in lofty or abstract concepts.
              </td>
              <td>
                <audio controls>
                  <source src="assets/audios/0_000307.wav" />
                </audio>
                <p>PromptTTS</p>
              </td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="container">
        <div class="content is-large">
          <h2 class="title is-3">Stage 2: Model training</h2>

          <p>
            During the model training phase, we utilize a pre-trained Whisper model to extract speech features and
            generate text transcriptions from the audio input. These speech features and the text embeddings (from the
            transcription and chat template) are concatenated before being fed into Llama3. We maintain a consistent
            template structure to prevent any mismatch. Finally, the model is trained to generate speech descriptions,
            effectively learning to interpret and articulate the nuances of spoken content based on both acoustic
            features and textual information.
          </p>

          <img src="assets/images/model_training.png" alt="" />
        </div>
      </div>
    </section>
  </head>

  <body></body>
</html>
